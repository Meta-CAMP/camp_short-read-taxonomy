'''Workflow for the CAMP short read taxonomy module.'''


from contextlib import redirect_stderr
import os
from os.path import abspath, basename, dirname, exists, join
import pandas as pd
import shutil
from utils import Workflow_Dirs, ingest_samples, scrub_fastq_captions, standardize_metaphlan, standardize_bracken, standardize_xtree, concat_tbls, extract_unclassified_names


# Load and/or make the working directory structure
dirs = Workflow_Dirs(config['work_dir'], 'short-read-taxonomy')


# Load sample names and input files 
SAMPLES = ingest_samples(config['samples'], dirs.TMP)
RANKS   = ['species', 'genus', 'family', 'order', 'class', 'phylum']
FQ_DIRS  = ['1', '2']

# Specify the location of any external resources and scripts
dirs_ext = config['ext'] # join(dirname(abspath(__file__)), 'ext')
dirs_scr = join(dirs_ext, 'scripts')


# --- Workflow output --- #


def read_masking(wildcards):
	sample = wildcards.sample
	fwd = join(dirs.TMP, sample + '_1.fastq.gz')
	rev = join(dirs.TMP, sample + '_2.fastq.gz')
	if bool(config['mask']):
		fwd = join(dirs.OUT, '0_masked_fastqs', sample + '_1.masked.fastq.gz')
		rev = join(dirs.OUT, '0_masked_fastqs', sample + '_2.masked.fastq.gz')
	return([fwd,rev])


def check_make(d):
    if not exists(d):
        os.makedirs(d)


def workflow_mode(wildcards):
	out = []
	if bool(config['metaphlan']):
		out.extend(expand(join(dirs.OUT, 'final_reports', 'metaphlan_{rank}.csv'), rank = RANKS))
		check_make(join(dirs.OUT, 'final_reports', 'unclassified', 'metaphlan'))
		out.extend(expand(join(dirs.OUT, 'final_reports', 'unclassified', 'metaphlan', '{sample}_{dir}.fastq.gz'), \
			sample = SAMPLES, dir = FQ_DIRS)),
	if bool(config['kraken2']):
		out.extend(expand(join(dirs.OUT, 'final_reports', 'kraken_bracken_{rank}.csv'), rank = RANKS))
		check_make(join(dirs.OUT,'final_reports','unclassified','kraken_bracken'))
		out.extend(expand(join(dirs.OUT, 'final_reports', 'unclassified', 'kraken_bracken', '{sample}_{dir}.fastq.gz'), \
			sample = SAMPLES, dir = FQ_DIRS)),
	if config['xtree']:
		XTREE = [g.strip() for g in config['xtree'].split(',')]
		for g in XTREE:
			out_dir = join(dirs.OUT, '3_xtree', g)
			if not exists(out_dir):
				os.makedirs(out_dir)
			if g == 'bacterial_archaeal':
				out.extend(expand(join(dirs.OUT,'final_reports','xtree_{rank}.csv'), rank = RANKS))
			else:
				out.append(join(dirs.OUT, '3_xtree', 'merged', g + '_ra.tsv'))
	return out


rule all:
	input:
		join(dirs.OUT, 'final_reports', 'complete.txt')


# --- Workflow steps --- #


rule mask_reads:
	input:
		fwd = join(dirs.TMP,'{sample}_1.fastq.gz'),
		rev = join(dirs.TMP,'{sample}_2.fastq.gz'),
	output:
		fwd = join(dirs.OUT, '0_masked_fastqs', '{sample}_1.masked.fastq.gz'),
		rev = join(dirs.OUT, '0_masked_fastqs', '{sample}_2.masked.fastq.gz'),
	log:
		join(dirs.LOG, 'masking', '{sample}.out'),
	threads:
		config['mask_reads_threads'],
	shell:
		"""
		bbmask.sh in={input.fwd} out={output.fwd} overwrite=t threads={threads} > {log} 2>&1
		bbmask.sh in={input.rev} out={output.rev} overwrite=t threads={threads} > {log} 2>&1
		"""	  


rule scrub_fastq_captions:
	input:
		join(dirs.TMP,'{sample}_{dir}.fastq.gz'),
	output:
		join(dirs.OUT,'1_metaphlan','{sample}_{dir}.fastq'),
	resources:
		mem_mb = config['scrub_fastq_mem_mb'],
	run:
		scrub_fastq_captions(str(input), str(output))


rule metaphlan:
	input:
		lambda wildcards: expand(join(dirs.OUT,'1_metaphlan','{sample}_{dir}.fastq'), sample = wildcards.sample, dir = FQ_DIRS),
	output:
		report = join(dirs.OUT,'1_metaphlan','raw_output','{sample}.metaphlan'),
		sam = join(dirs.OUT,'1_metaphlan','raw_output','{sample}.sam'),
	log:
		join(dirs.LOG, 'metaphlan', '{sample}.out'),
	conda:
		join(config['env_yamls'], 'metaphlan.yaml'),
	threads:
		config['metaphlan_threads'],
	resources:
		mem_mb = config['metaphlan_mem_mb'],
	params:
		prefix = join(dirs.OUT,'1_metaphlan','raw_output', '{sample}'),
		database = config['metaphlan_database'],
	shell:
		"""
		metaphlan --nproc {threads} -t rel_ab_w_read_stats --force \
			--bowtie2db {params.database} \
			--bowtie2out {params.prefix}.bowtie2.bz2 \
			-o {output.report} --samout {output.sam} \
			--input_type fastq $(echo {input} | sed 's/ /,/g') > {log} 2>&1
		"""


rule standardize_metaphlan:
	input:
		join(dirs.OUT,'1_metaphlan', 'raw_output','{sample}.metaphlan'),
	output: 
		join(dirs.OUT,'1_metaphlan', 'standardized', '{sample}_species.csv'),
		join(dirs.OUT, '1_metaphlan', 'standardized', '{sample}_genus.csv'),
		join(dirs.OUT, '1_metaphlan', 'standardized', '{sample}_family.csv'),
		join(dirs.OUT, '1_metaphlan', 'standardized', '{sample}_order.csv'),
		join(dirs.OUT, '1_metaphlan', 'standardized', '{sample}_class.csv'),
		join(dirs.OUT, '1_metaphlan', 'standardized', '{sample}_phylum.csv'),
	params:
		out_dir = join(dirs.OUT,'1_metaphlan','standardized'),
		min_abd = config['min_rel_abund'],
	run:
		if not exists(str(params.out_dir)):
			os.makedirs(str(params.out_dir))
		standardize_metaphlan(str(input), str(params.out_dir), int(params.min_abd))
# lambda wildcards: expand(join(dirs.OUT,'1_metaphlan', 'standardized', '{sample}_{rank}.csv'), sample = wildcards.sample, rank = RANKS),


rule merge_metaphlan:
	input:
		lambda wildcards: expand(join(dirs.OUT, '1_metaphlan', 'standardized', '{sample}_{rank}.csv'), sample = SAMPLES, rank = wildcards.rank),
	output: 
		join(dirs.OUT, 'final_reports', 'metaphlan_{rank}.csv'),
	params:
		out_dir = join(dirs.OUT, 'final_reports'),
	run:
		concat_tbls(input, str(output))


rule extract_unclassified_metaphlan:
	input:
		join(dirs.OUT,'1_metaphlan','raw_output','{sample}.sam'),
	output:
		fwd = join(dirs.OUT,'final_reports','unclassified', 'metaphlan', '{sample}_1.fastq.gz'),
		rev = join(dirs.OUT,'final_reports','unclassified', 'metaphlan', '{sample}_2.fastq.gz'),
		unp = join(dirs.OUT,'final_reports','unclassified', 'metaphlan', '{sample}_unp.fastq.gz'),
	threads:
		config['metaphlan_threads'],
	resources:
		mem_mb = config['metaphlan_mem_mb'],
	params:
		join(dirs.OUT, 'final_reports', 'unclassified'),
	shell:
		"""
		samtools fastq -@ {threads} -f 4 {input} -1 {output.fwd} -2 {output.rev} -s {output.unp}
		"""


rule kraken2:
	input:
		read_masking,
	output:
		kraken = join(dirs.OUT, '2_kraken2', 'raw_kraken', '{sample}', 'kraken.tsv'),
		kreport = join(dirs.OUT,'2_kraken2', 'raw_kraken', '{sample}', 'kreport.tsv')
	log:
		join(dirs.LOG, 'kraken2', '{sample}.out'),
	threads: 
		config['kraken2_threads'],
	resources:
		mem_mb = config['kraken2_mem_mb'],
	params:
		out_dir = join(dirs.OUT, '2_kraken2', 'raw_kraken', '{sample}'),
		database = config['kraken_bracken_database'],
		kraken_exec = config['kraken2_executable'],
	shell:
		"""
		mkdir -p {params.out_dir}
		{params.kraken_exec} --db {params.database} --threads {threads} --report {output.kreport} --output {output.kraken} --paired {input} > {log} 2>&1
		"""	


rule bracken:
	input:
		join(dirs.OUT, '2_kraken2', 'raw_kraken', '{sample}', 'kreport.tsv'),
	output:
		join(dirs.OUT, '2_kraken2', 'raw_bracken','{sample}', '{rank}.tsv'),
	log:
		join(dirs.LOG, 'bracken', '{sample}.{rank}.out'),
	conda:
		join(config['env_yamls'], 'bracken.yaml'),
	params:
		out_dir = join(dirs.OUT,'2_kraken2', 'raw_bracken', '{sample}'),
		rank = lambda wildcards: '{}'.format(wildcards.rank)[:1].capitalize(),
		read_len = config['read_len'],
		database = config['kraken_bracken_database'],
	shell:
		"""
		mkdir -p {params.out_dir}
		bracken -r {params.read_len} -d {params.database}  -l {params.rank} -i {input} -o {output} > {log} 2>&1
		"""


rule standardize_bracken:
	input:
		join(dirs.OUT, '2_kraken2', 'raw_bracken', '{sample}', '{rank}.tsv'),
	output: 
		join(dirs.OUT, '2_kraken2', 'standardized', '{sample}_{rank}.csv'),
	params:
		out_dir = join(dirs.OUT, '2_kraken2', 'standardized'),
		min_abd = config['min_rel_abund'],		
	run:
		if not exists(str(params.out_dir)):
			os.makedirs(str(params.out_dir))
		standardize_bracken(str(input), str(params.out_dir), int(params.min_abd))


rule merge_bracken:
	input:
		lambda wildcards: expand(join(dirs.OUT, '2_kraken2', 'standardized', '{sample}_{rank}.csv'), sample = SAMPLES, rank = wildcards.rank),
	output: 
		join(dirs.OUT, 'final_reports', 'kraken_bracken_{rank}.csv'),
	params:
		out_dir = join(dirs.OUT, 'final_reports'),
	run:
		concat_tbls(input, str(output))


rule extract_unclassified_names:
	input: 
		join(dirs.OUT, '2_kraken2', 'raw_kraken', '{sample}', 'kraken.tsv'),
	output:
		join(dirs.OUT, '2_kraken2', 'raw_kraken', '{sample}', 'unclassified.txt'),
	run:
		extract_unclassified_names(str(input), str(output))


rule extract_unclassified_kraken:
	input:
		fq = join(dirs.TMP,'{sample}_{dir}.fastq.gz'),
		unc = join(dirs.OUT, '2_kraken2', 'raw_kraken', '{sample}', 'unclassified.txt'),
	output: 
		join(dirs.OUT, 'final_reports', 'unclassified', 'kraken_bracken', '{sample}_{dir}.fastq.gz'),
	resources:
		mem_mb = config['kraken2_mem_mb'],
	shell:
		"""
		seqtk subseq {input.fq} {input.unc} | gzip > {output}
		"""


rule make_xtree_input:
	input:
		lambda wildcards: expand(join(dirs.OUT,'1_metaphlan','{sample}_{dir}.fastq'), sample = wildcards.sample, dir = FQ_DIRS),
	output:
		join(dirs.OUT, '3_xtree', '{sample}.fastq'),
	threads: 
		config['xtree_threads'],
	resources:
		mem_mb = config['xtree_mem_mb'],
	shell:
		"""
		cat {input} > {output}
		"""


rule xtree:
	input:
		fq = join(dirs.OUT, '3_xtree', '{sample}.fastq'),
	output:
		ref = join(dirs.OUT, '3_xtree', '{xtree_group}','{sample}.ref'),
		cov = join(dirs.OUT, '3_xtree', '{xtree_group}','{sample}.cov'),
	log:
		join(dirs.LOG, 'xtree', '{sample}.{xtree_group}.out'),
	threads: 
		config['xtree_threads'],
	resources:
		mem_mb = config['xtree_mem_mb'],
	params:
		out_dir = join(dirs.OUT, '3_xtree', '{xtree_group}'),
		ext_script = join(dirs_scr, 'run_xtree.sh'),
		xtree_exec = config['xtree_executable'],
		db = lambda wildcards: config['{}_database'.format(wildcards.xtree_group)],
		prefix = join(dirs.OUT, '3_xtree', '{xtree_group}','{sample}'),
	shell:
		"""
		mkdir -p {params.out_dir}
		{params.xtree_exec} --seqs {input.fq} --threads {threads} --db {params.db} --ref-out {output.ref} --cov-out {output.cov} --redistribute > {log} 2>&1
		"""	
# {params.ext_script} {params.xtree_exec} {threads} {input.fq} {params.db} {params.prefix} > {log} 2>&1


rule merge_xtree_outputs:
	input:
		lambda wildcards: expand(join(dirs.OUT, '3_xtree', '{xtree_group}', '{sample}.cov'), xtree_group = wildcards.xtree_group, sample = SAMPLES),
	output:
		join(dirs.OUT, '3_xtree', 'merged', '{xtree_group}_ra.tsv'),
	params:
		ext_script = join(dirs_scr, 'post_process_xtree.R'),
		indir = join(dirs.OUT, '3_xtree', '{xtree_group}'),
		xtree_group = '{xtree_group}',
		out_dir = join(dirs.OUT,'3_xtree', 'merged'),
		thresh = config['min_rel_abund'],
		hthresh = config['hthresh'],
		uthresh = config['uthresh'],
		mappings = join(dirs_ext, 'xtree_all_db_mapping'),
	shell: 
		"""
		Rscript {params.ext_script} {params.indir} {params.thresh} {params.hthresh} {params.uthresh} {params.out_dir} {params.xtree_group} {params.mappings}
		"""


rule standardize_xtree:
	input:
		join(dirs.OUT, '3_xtree', 'merged', 'bacterial_archaeal_ra.tsv'),
	output:
		join(dirs.OUT,'final_reports','xtree_species.csv'),
		join(dirs.OUT,'final_reports','xtree_genus.csv'),
		join(dirs.OUT,'final_reports','xtree_family.csv'),
		join(dirs.OUT,'final_reports','xtree_order.csv'),
		join(dirs.OUT,'final_reports','xtree_class.csv'),
		join(dirs.OUT,'final_reports','xtree_phylum.csv'),
		# *[join(dirs.OUT, 'final_reports', 'xtree_{rank}.csv') for rank in RANKS],
		# lambda wildcards: expand(join(dirs.OUT, 'final_reports', 'xtree_{rank}.csv'), rank = RANKS),
	params:
		out_dir = join(dirs.OUT, 'final_reports'),
		ncbi_taxid = config['ncbi_tax_names'],
		uthresh = config['uthresh'],
	run:
		standardize_xtree(str(input), str(params.out_dir), str(params.ncbi_taxid), float(params.uthresh))


rule make_config:
	input:
		workflow_mode,
	output:
		join(dirs.OUT, 'final_reports', 'complete.txt'),
	params:
		out_dir = join(dirs.OUT, 'final_reports'),
	run:
		for i in input: # Copy incompatible XTree reports from non-bacterial/archaeal superkingdoms to the output directory
			if any(True for k in ['protozoa_fungi', 'viral'] if k in i):
				shutil.copy(str(i), join(str(params.out_dir), 'xtree_' + basename(i)))
		open(str(output), 'w').close()



