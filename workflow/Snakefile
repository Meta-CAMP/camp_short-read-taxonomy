'''Workflow for the CAMP short read taxonomy module.'''


from contextlib import redirect_stderr
import os
from os.path import abspath, basename, dirname, exists, join
import pandas as pd
import shutil
from utils import Workflow_Dirs, ingest_samples, scrub_fastq_captions


# Load and/or make the working directory structure
dirs = Workflow_Dirs(config['work_dir'], 'short-read-taxonomy')


# Load sample names and input files 
SAMPLES = ingest_samples(config['samples'], dirs.TMP)
RANKS   = ['species', 'genus', 'family', 'order', 'class', 'phylum']
RANK_ABBREV = { 'species' : 'S', 'genus' : 'G', 'family' : 'F', 'order' : 'O', 'class' : 'C', 'phylum' : 'P'}

# Specify the location of any external resources and scripts
dirs_ext = config['ext'] # join(dirname(abspath(__file__)), 'ext')
dirs_scr = join(dirs_ext, 'scripts')


# --- Workflow output --- #


def read_masking(wildcards):
	sample = wildcards.sample
	fwd = join(dirs.TMP, sample + '_1.fastq.gz')
	rev = join(dirs.TMP, sample + '_2.fastq.gz')
	if bool(config['mask']):
		fwd = join(dirs.OUT, '0_masked_fastqs', sample + '_1.masked.fastq.gz')
		rev = join(dirs.OUT, '0_masked_fastqs', sample + '_2.masked.fastq.gz')
	return([fwd,rev])


def workflow_mode(wildcards):
	out = []
	if bool(config['metaphlan']):
		out.append(join(dirs.OUT,'1_metaphlan','metaphlan.tsv'))
	if bool(config['kraken2']):
		out.extend(expand(join(dirs.OUT, '2_kraken2', 'merged', \
			'bracken_{rank}.tsv'), rank = RANKS))
	if config['xtree']:
		XTREE = [g.strip() for g in config['xtree'].split(',')]
		for g in XTREE:
			out_dir = join(dirs.OUT, '3_xtree', g)
			if not exists(out_dir):
				os.makedirs(out_dir)
		out.extend(expand(join(dirs.OUT, '3_xtree', 'merged', \
			'{xtree_group}_ra.tsv'), xtree_group = XTREE))
	return(out)


rule all:
	input:
		join(dirs.OUT, 'final_reports', 'complete.txt')


# --- Workflow steps --- #


rule mask_reads:
	input:
		fwd = join(dirs.TMP,'{sample}_1.fastq.gz'),
		rev = join(dirs.TMP,'{sample}_2.fastq.gz'),
	output:
		fwd = join(dirs.OUT, '0_masked_fastqs', '{sample}_1.masked.fastq.gz'),
		rev = join(dirs.OUT, '0_masked_fastqs', '{sample}_2.masked.fastq.gz'),
	log:
		join(dirs.LOG, 'masking', '{sample}.out'),
	threads:
		config['mask_reads_threads'],
	shell:
		"""
		bbmask.sh in={input.fwd} out={output.fwd} overwrite=t threads={threads} > {log} 2>&1
		bbmask.sh in={input.rev} out={output.rev} overwrite=t threads={threads} > {log} 2>&1
		"""	  


rule scrub_fastq_captions:
	input:
		join(dirs.TMP,'{sample}_{dir}.fastq.gz'),
	output:
		join(dirs.OUT,'1_metaphlan','{sample}_{dir}.fastq'),
	resources:
		mem_mb = config['scrub_fastq_mem_mb'],
	run:
		scrub_fastq_captions(str(input), str(output))


rule metaphlan:
	input:
		lambda wildcards: expand(join(dirs.OUT,'1_metaphlan','{sample}_{dir}.fastq'), sample = wildcards.sample, dir = ['1', '2']),
	output:
		join(dirs.OUT,'1_metaphlan','raw_output','{sample}.metaphlan'),
	log:
		join(dirs.LOG, 'metaphlan', '{sample}.out'),
	conda:
		join(config['env_yamls'], 'metaphlan.yaml'),
	threads:
		config['metaphlan_threads'],
	resources:
		mem_mb = config['metaphlan_mem_mb'],
	params:
		out_dir = join(dirs.OUT,'1_metaphlan','raw_output'),
		prefix=join(dirs.OUT,'1_metaphlan','raw_output', '{sample}'),
		database = config['metaphlan_database'],
	shell:
		"""
		rm -r {params.out_dir}
		mkdir -p {params.out_dir}
		metaphlan --nproc {threads} --bowtie2db {params.database} --input_type fastq --force  -o {output} --bowtie2out {params.prefix}.bowtie2.bz2 $(echo {input} | sed 's/ /,/g') > {log} 2>&1
		""" 


rule merge_metaphlan_outputs:
	input:
		expand(join(dirs.OUT,'1_metaphlan','raw_output','{sample}.metaphlan'),sample = SAMPLES),
	output:
		join(dirs.OUT,'1_metaphlan','metaphlan.tsv'),
	conda:
		join(config['env_yamls'], 'metaphlan.yaml'),
	params: 
		ext_script = join(dirs_scr, 'merge_metaphlan_tables.py'),
	shell:
		"""
		python3 {params.ext_script} {input} > {output}
		"""	


rule kraken2:
	input:
		read_masking,
	output:
		kraken = join(dirs.OUT,'2_kraken2', 'raw_kraken', '{sample}', 'kraken.tsv'),
		kreport = join(dirs.OUT,'2_kraken2', 'raw_kraken', '{sample}', 'kreport.tsv')
	log:
		join(dirs.LOG, 'kraken2', '{sample}.out'),
	threads: 
		config['kraken2_threads'],
	resources:
		mem_mb = config['kraken2_mem_mb'],
	params:
		out_dir = join(dirs.OUT, '2_kraken2', 'raw_kraken', '{sample}'),
		database = config['kraken_bracken_database'],
		kraken_exec = config['kraken2_executable'],
	shell:
		"""
		rm -r {params.out_dir}
		mkdir -p {params.out_dir}
		{params.kraken_exec} --db {params.database} --threads {threads} --report {output.kreport} --output {output.kraken} --paired {input} > {log} 2>&1
		"""	


rule bracken:
	input:
		join(dirs.OUT, '2_kraken2', 'raw_kraken', '{sample}', 'kreport.tsv'),
	output:
		join(dirs.OUT, '2_kraken2', 'raw_bracken','{sample}', '{rank}.tsv'),
	log:
		join(dirs.LOG, 'bracken', '{sample}.{rank}.out'),
	conda:
		join(config['env_yamls'], 'bracken.yaml'),
	params:
		out_dir = join(dirs.OUT,'2_kraken2', 'raw_bracken', '{sample}'),
		rank = lambda wildcards: '{}'.format(wildcards.rank)[:1].capitalize(),
		read_len = config['read_len'],
		database = config['kraken_bracken_database'],
	shell:
		"""
		mkdir -p {params.out_dir}
		bracken -r {params.read_len} -d {params.database}  -l {params.rank} -i {input} -o {output} > {log} 2>&1
		"""


rule merge_bracken_outputs:
	input:
		lambda wildcards: expand(join(dirs.OUT, '2_kraken2', \
			'raw_bracken','{sample}', '{rank}.tsv'), \
			sample = SAMPLES, rank = wildcards.rank),
	output:
		join(dirs.OUT, '2_kraken2', 'merged','bracken_{rank}.tsv')
	conda:
		join(config['env_yamls'], 'bracken.yaml'),
	params:
		out_dir = join(dirs.OUT, '2_kraken2', 'merged'),
		ext_script = join(dirs_scr, 'combine_bracken_outputs.py'),
	shell:
		"""
		mkdir -p {params.out_dir}
		python {params.ext_script} --files {input} -o {output}
		"""


rule make_xtree_input:
	input:
		lambda wildcards: expand(join(dirs.OUT,'1_metaphlan','{sample}_{dir}.fastq'), sample = wildcards.sample, dir = ['1', '2']),
	output:
		join(dirs.OUT, '3_xtree', '{sample}.fastq'),
	threads: 
		config['xtree_threads'],
	resources:
		mem_mb = config['xtree_mem_mb'],
	shell:
		"""
		cat {input} > {output}
		"""


rule xtree:
	input:
		join(dirs.OUT, '3_xtree', '{xtree_group}'),
		fq = join(dirs.OUT, '3_xtree', '{sample}.fastq'),
	output:
		ref = join(dirs.OUT, '3_xtree', '{xtree_group}','{sample}.ref'),
		cov = join(dirs.OUT, '3_xtree', '{xtree_group}','{sample}.cov'),
	log:
		join(dirs.LOG, 'xtree', '{sample}.{xtree_group}.out'),
	threads: 
		config['xtree_threads'],
	resources:
		mem_mb = config['xtree_mem_mb'],
	params:
		out_dir = join(dirs.OUT, '3_xtree', '{xtree_group}'),
		ext_script = join(dirs_scr, 'run_xtree.sh'),
		xtree_exec = config['xtree_executable'],
		db = lambda wildcards: config['xtree_{}_database'.format(wildcards.xtree_group)],
		prefix = join(dirs.OUT, '3_xtree', '{xtree_group}','{sample}'),
	shell:
		"""
		cd {params.out_dir}
		{params.ext_script} {params.xtree_exec} {threads} {input.fq} {params.db} {params.prefix} > {log} 2>&1
		"""	
# {params.xtree_exec} --seqs {input.fq} --threads {threads} --db {params.db} --ref-out {output.ref} --cov-out {output.cov} --redistribute > {log} 2>&1


rule merge_xtree_outputs:
	input:
		lambda wildcards: expand(join(dirs.OUT, '3_xtree', '{xtree_group}', '{sample}.cov'), xtree_group = wildcards.xtree_group, sample = SAMPLES),
	output:
		expand(join(dirs.OUT, '3_xtree', 'merged', 'xtree_{{xtree_group}}_{out_type}.tsv'), out_type = ['counts', 'counts_raw', 'ra', 'ra_raw', 'coverages', 'unique_coverages'])
	params:
		ext_script = join(dirs_scr, 'post_process_xtree.R'),
		indir = join(dirs.OUT, '3_xtree', '{xtree_group}'),
		xtree_group = '{xtree_group}',
		out_dir = join(dirs.OUT,'3_xtree', 'merged'),
		thresh = config['thresh'],
		hthresh = config['hthresh'],
		uthresh = config['uthresh'],
		mappings = join(dirs_ext, 'xtree_all_db_mapping'),
	shell: 
		"""
		mkdir -p {params.out_dir}
		Rscript {params.ext_script} {params.indir} {params.thresh} {params.hthresh} {params.uthresh} {params.out_dir} {params.xtree_group} {params.mappings}
		"""


rule make_config:
	input:
		workflow_mode,
	output:
		join(dirs.OUT, 'final_reports', 'complete.txt')
	params:
		out_dir = join(dirs.OUT, 'final_reports'),
	run:
		out = []
		for i in input:
			shutil.copy(str(i), join(params.out_dir, basename(i)))
		open(str(output), 'a').close()



